{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.ticker as ticker\n",
    "def add_weekday_month_names(df):\n",
    "    # Ensure 'created_at' is a datetime column\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "    # Extract the weekday and month\n",
    "    df['weekday'] = df['created_at'].dt.weekday\n",
    "    df['month'] = df['created_at'].dt.month\n",
    "\n",
    "    # Weekday and month name mappings\n",
    "    weekday_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', \n",
    "                     4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "    month_names = {1: 'January', 2: 'February', 3: 'March', 4: 'April', \n",
    "                   5: 'May', 6: 'June', 7: 'July', 8: 'August', \n",
    "                   9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n",
    "\n",
    "    # Convert weekday and month numbers to names\n",
    "    df['weekday'] = df['weekday'].map(weekday_names)\n",
    "    df['month'] = df['month'].map(month_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 16\n",
    "mpl.rcParams['ytick.labelsize'] = 16\n",
    "mpl.rcParams['legend.fontsize'] = 16\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_start_date = '2013-01-01'\n",
    "analysis_end_date = '2015-04-30'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emot = pd.read_csv('packages_emotions.csv')\n",
    "df_sentiment= pd.read_csv('packages_sentiment.csv',index_col=[0])\n",
    "df_topic= pd.read_csv('packages_topics.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emot = df_emot[['created_at','test_id','cleaned_text','impressions','clicks','ctr','emotion','confidence']]\n",
    "df_emot = df_emot.rename(columns={'confidence': 'emotion_score'})\n",
    "df_topic = df_topic.rename(columns={'labels': 'topic','scores':'topic_score'})\n",
    "df_sentiment['sentiment'] = df_sentiment['sentiment'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>test_id</th>\n",
       "      <th>impressions</th>\n",
       "      <th>clicks</th>\n",
       "      <th>ctr</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48716</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>5143608d220cb800020009e2</td>\n",
       "      <td>6983</td>\n",
       "      <td>105</td>\n",
       "      <td>0.015037</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.991908</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.998309</td>\n",
       "      <td>A Culture That Refuses To Speak About Pregnanc...</td>\n",
       "      <td>Society and Social Issues</td>\n",
       "      <td>0.553050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133752</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>51436070220cb8000200074e</td>\n",
       "      <td>3486</td>\n",
       "      <td>122</td>\n",
       "      <td>0.034997</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.986707</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.997439</td>\n",
       "      <td>A Woman Lived Through Something Awful Now Shes...</td>\n",
       "      <td>Society and Social Issues</td>\n",
       "      <td>0.381437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48810</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>5143608d220cb800020009e2</td>\n",
       "      <td>7044</td>\n",
       "      <td>77</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.990454</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.998423</td>\n",
       "      <td>What It Means To Change The Perception Of Who ...</td>\n",
       "      <td>Society and Social Issues</td>\n",
       "      <td>0.605390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50444</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>5143608d220cb800020009e2</td>\n",
       "      <td>6986</td>\n",
       "      <td>109</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.994656</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.998072</td>\n",
       "      <td>Watch One Blogger Calmly Explain Why Shes Sick...</td>\n",
       "      <td>Pop Culture, Media and Entertainment</td>\n",
       "      <td>0.506504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133930</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>51436070220cb8000200074e</td>\n",
       "      <td>3364</td>\n",
       "      <td>90</td>\n",
       "      <td>0.026754</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.995158</td>\n",
       "      <td>Maybe Shes Born With It Or She Just Bleached T...</td>\n",
       "      <td>Pop Culture, Media and Entertainment</td>\n",
       "      <td>0.463655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25969</th>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>55418d7b313132002c070000</td>\n",
       "      <td>2043</td>\n",
       "      <td>22</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.823319</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.706596</td>\n",
       "      <td>When Elizabeth Warren Was 12 Years Old Somethi...</td>\n",
       "      <td>Relationships, Gender and Family</td>\n",
       "      <td>0.438197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25970</th>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>55418d7b313132002c070000</td>\n",
       "      <td>1953</td>\n",
       "      <td>18</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.823319</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.706596</td>\n",
       "      <td>Theres A Reason Elizabeth Warren Thinks The Mi...</td>\n",
       "      <td>Society and Social Issues</td>\n",
       "      <td>0.690894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25962</th>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>55417f91343635002c7e0000</td>\n",
       "      <td>2062</td>\n",
       "      <td>39</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.823319</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.706596</td>\n",
       "      <td>Beyonc√©s Epic Girl Power Anthem That Kinda Sor...</td>\n",
       "      <td>Pop Culture, Media and Entertainment</td>\n",
       "      <td>0.683434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25963</th>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>55417f91343635002c7e0000</td>\n",
       "      <td>1962</td>\n",
       "      <td>31</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.823319</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.706596</td>\n",
       "      <td>Inevitably Someone Will Say But My Juice Clean...</td>\n",
       "      <td>Authenticity, Lifestyle and Health</td>\n",
       "      <td>0.622102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25961</th>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>55417382313766001c440000</td>\n",
       "      <td>2067</td>\n",
       "      <td>24</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.823319</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.706596</td>\n",
       "      <td>Beyonc√©s Epic Girl Power Anthem That Kinda Sor...</td>\n",
       "      <td>Pop Culture, Media and Entertainment</td>\n",
       "      <td>0.683434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150817 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       created_at                   test_id  impressions  clicks       ctr  \\\n",
       "48716  2013-01-24  5143608d220cb800020009e2         6983     105  0.015037   \n",
       "133752 2013-01-24  51436070220cb8000200074e         3486     122  0.034997   \n",
       "48810  2013-01-24  5143608d220cb800020009e2         7044      77  0.010931   \n",
       "50444  2013-01-24  5143608d220cb800020009e2         6986     109  0.015603   \n",
       "133930 2013-01-24  51436070220cb8000200074e         3364      90  0.026754   \n",
       "...           ...                       ...          ...     ...       ...   \n",
       "25969  2015-04-30  55418d7b313132002c070000         2043      22  0.010768   \n",
       "25970  2015-04-30  55418d7b313132002c070000         1953      18  0.009217   \n",
       "25962  2015-04-30  55417f91343635002c7e0000         2062      39  0.018914   \n",
       "25963  2015-04-30  55417f91343635002c7e0000         1962      31  0.015800   \n",
       "25961  2015-04-30  55417382313766001c440000         2067      24  0.011611   \n",
       "\n",
       "       emotion  emotion_score sentiment  sentiment_score  \\\n",
       "48716      joy       0.991908  negative         0.998309   \n",
       "133752     joy       0.986707  positive         0.997439   \n",
       "48810      joy       0.990454  negative         0.998423   \n",
       "50444      joy       0.994656  negative         0.998072   \n",
       "133930     joy       0.975622  positive         0.995158   \n",
       "...        ...            ...       ...              ...   \n",
       "25969    anger       0.823319  positive         0.706596   \n",
       "25970    anger       0.823319  positive         0.706596   \n",
       "25962    anger       0.823319  positive         0.706596   \n",
       "25963    anger       0.823319  positive         0.706596   \n",
       "25961    anger       0.823319  positive         0.706596   \n",
       "\n",
       "                                             cleaned_text  \\\n",
       "48716   A Culture That Refuses To Speak About Pregnanc...   \n",
       "133752  A Woman Lived Through Something Awful Now Shes...   \n",
       "48810   What It Means To Change The Perception Of Who ...   \n",
       "50444   Watch One Blogger Calmly Explain Why Shes Sick...   \n",
       "133930  Maybe Shes Born With It Or She Just Bleached T...   \n",
       "...                                                   ...   \n",
       "25969   When Elizabeth Warren Was 12 Years Old Somethi...   \n",
       "25970   Theres A Reason Elizabeth Warren Thinks The Mi...   \n",
       "25962   Beyonc√©s Epic Girl Power Anthem That Kinda Sor...   \n",
       "25963   Inevitably Someone Will Say But My Juice Clean...   \n",
       "25961   Beyonc√©s Epic Girl Power Anthem That Kinda Sor...   \n",
       "\n",
       "                                       topic  topic_score  \n",
       "48716              Society and Social Issues     0.553050  \n",
       "133752             Society and Social Issues     0.381437  \n",
       "48810              Society and Social Issues     0.605390  \n",
       "50444   Pop Culture, Media and Entertainment     0.506504  \n",
       "133930  Pop Culture, Media and Entertainment     0.463655  \n",
       "...                                      ...          ...  \n",
       "25969       Relationships, Gender and Family     0.438197  \n",
       "25970              Society and Social Issues     0.690894  \n",
       "25962   Pop Culture, Media and Entertainment     0.683434  \n",
       "25963     Authenticity, Lifestyle and Health     0.622102  \n",
       "25961   Pop Culture, Media and Entertainment     0.683434  \n",
       "\n",
       "[150817 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_emot.merge(df_sentiment, left_index=True, right_index=True)\n",
    "df_merged = df_merged.merge(df_topic,left_index=True,right_index=True)\n",
    "#df_merged = df_merged.rename(columns={'cleaned_text_x': 'cleaned_text'})\n",
    "df_merged = df_merged.drop(['cleaned_text_x','cleaned_text_y'],axis=1)\n",
    "df_merged = df_merged.sort_values(by=\"created_at\")\n",
    "df_merged['created_at']= pd.to_datetime(df_merged['created_at'], format='%Y-%m-%d')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['sentiment'] = np.where((df_merged['sentiment_score'] <= 0.9), 'unclassified sentiment', df_merged['sentiment'])\n",
    "df_merged['emotion'] = np.where((df_merged['emotion_score'] <= 0.75) , 'unclassified emotion', df_merged['emotion'])\n",
    "df_merged['topic'] = np.where((df_merged['topic_score'] <= 0.4) , 'unclassified topic', df_merged['topic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rolling_sentiment_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22408/1213326270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Calculate rolling mean and standard deviation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mrolling_sentiment_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrolling_sentiment_mean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ffill'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Forward fill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mrolling_sentiment_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrolling_sentiment_std\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ffill'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rolling_sentiment_mean' is not defined"
     ]
    }
   ],
   "source": [
    "daily_sentiment = df_merged.groupby(['created_at', 'sentiment']).size().unstack(fill_value=0)\n",
    "daily_emotion = df_merged.groupby(['created_at', 'emotion']).size().unstack(fill_value=0)\n",
    "daily_topic = df_merged.groupby(['created_at', 'topic']).size().unstack(fill_value=0)\n",
    "\n",
    "daily_sentiment_proportions = daily_sentiment.div(daily_sentiment.sum(axis=1), axis=0)\n",
    "daily_emotion_proportions = daily_emotion.div(daily_emotion.sum(axis=1), axis=0)\n",
    "daily_topic_proportions = daily_topic.div(daily_topic.sum(axis=1), axis=0)\n",
    "\n",
    "# Define the rolling window size\n",
    "window_size = 30\n",
    "\n",
    "# Calculate rolling mean and standard deviation\n",
    "rolling_sentiment_mean = rolling_sentiment_mean.fillna(method='ffill')  # Forward fill\n",
    "rolling_sentiment_std = rolling_sentiment_std.fillna(method='ffill')\n",
    "\n",
    "# You can do the same for emotion and topic data\n",
    "rolling_emotion_mean = rolling_emotion_mean.fillna(method='ffill')\n",
    "rolling_emotion_std = rolling_emotion_std.fillna(method='ffill')\n",
    "\n",
    "rolling_topic_mean = rolling_topic_mean.fillna(method='ffill')\n",
    "rolling_topic_std = rolling_topic_std.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def clean_data(data):\n",
    "    # Replace inf/-inf with NaN and forward/backward fill\n",
    "    return data.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "def plot_column(ax, column_data, std_data, title, font_size):\n",
    "    # Clean the data\n",
    "    column_data = clean_data(column_data)\n",
    "    std_data = clean_data(std_data)\n",
    "\n",
    "    # Check if data still contains NaNs or infinities\n",
    "    if not column_data.isna().any() and not np.isinf(column_data).any():\n",
    "        # Perform Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(column_data)\n",
    "        p_value = adf_result[1]\n",
    "        adf_label = f'{title} (ADF p-value: {p_value:.2f})'\n",
    "    else:\n",
    "        adf_label = f'{title} (ADF test not performed due to NaNs/Infinities)'\n",
    "\n",
    "    # Ensuring that standard deviation doesn't go below zero\n",
    "    lower_std = column_data - std_data\n",
    "    lower_std[lower_std < 0] = 0\n",
    "\n",
    "    # Plotting the mean and standard deviation\n",
    "    ax.plot(column_data, label=adf_label)\n",
    "    ax.fill_between(column_data.index, lower_std, column_data + std_data, alpha=0.2)\n",
    "\n",
    "def plot_with_rolling_mean(ax, mean_data, std_data, title, font_size):\n",
    "    for column in mean_data.columns:\n",
    "        plot_column(ax, mean_data[column], std_data[column], f\"{title} - {column}\", font_size)\n",
    "\n",
    "    # Setting the title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Proportions')\n",
    "\n",
    "    # Adding the legend\n",
    "    ax.legend()\n",
    "# Example use, replace with your actual data\n",
    "daily_sentiment = df_merged.groupby(['created_at', 'sentiment']).size().unstack(fill_value=0)\n",
    "daily_emotion = df_merged.groupby(['created_at', 'emotion']).size().unstack(fill_value=0)\n",
    "daily_topic = df_merged.groupby(['created_at', 'topic']).size().unstack(fill_value=0)\n",
    "\n",
    "daily_sentiment_proportions = daily_sentiment.div(daily_sentiment.sum(axis=1), axis=0)\n",
    "daily_emotion_proportions = daily_emotion.div(daily_emotion.sum(axis=1), axis=0)\n",
    "daily_topic_proportions = daily_topic.div(daily_topic.sum(axis=1), axis=0)\n",
    "\n",
    "# Define the rolling window size\n",
    "window_size = 30\n",
    "\n",
    "# Calculate rolling mean and standard deviation\n",
    "rolling_sentiment_mean = rolling_sentiment_mean.fillna(method='ffill')  # Forward fill\n",
    "rolling_sentiment_std = rolling_sentiment_std.fillna(method='ffill')\n",
    "\n",
    "# You can do the same for emotion and topic data\n",
    "rolling_emotion_mean = rolling_emotion_mean.fillna(method='ffill')\n",
    "rolling_emotion_std = rolling_emotion_std.fillna(method='ffill')\n",
    "\n",
    "rolling_topic_mean = rolling_topic_mean.fillna(method='ffill')\n",
    "rolling_topic_std = rolling_topic_std.fillna(method='ffill')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ... (Include the definitions of clean_data and plot_with_rolling_mean functions here)\n",
    "\n",
    "# Assuming you have your dataframes ready\n",
    "# rolling_sentiment_mean, rolling_sentiment_std, etc.\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 20), gridspec_kw={'height_ratios': [1, 1, 1]})\n",
    "\n",
    "# Adjust font sizes for better readability\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=16)\n",
    "plt.rc('axes', labelsize=16)\n",
    "plt.rc('legend', fontsize=13)\n",
    "\n",
    "# Plotting rolling mean for sentiment, emotion, and topic\n",
    "plot_with_rolling_mean(axs[0], rolling_sentiment_mean, rolling_sentiment_std, 'Sentiment Proportions', 16)\n",
    "plot_with_rolling_mean(axs[1], rolling_emotion_mean, rolling_emotion_std, 'Emotion Proportions', 16)\n",
    "plot_with_rolling_mean(axs[2], rolling_topic_mean, rolling_topic_std, 'Topic Proportions', 16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('proportions_rolling_means.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12, 16), gridspec_kw={'height_ratios': [1, 1, 1]})\n",
    "\n",
    "# Adjust font sizes for better readability\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=16)\n",
    "plt.rc('axes', labelsize=16)\n",
    "plt.rc('legend', fontsize=16)\n",
    "\n",
    "# Histogram for sentiment\n",
    "df_merged['sentiment'].value_counts(normalize=True).plot(kind='bar', ax=axs[0], color='green')\n",
    "axs[0].set_title('Sentiment Distribution')\n",
    "axs[0].set_ylabel('Normalized Count')\n",
    "axs[0].set_xticklabels(split_labels(df_merged['sentiment'].value_counts().index), rotation=45)\n",
    "\n",
    "# Histogram for emotion\n",
    "df_merged['emotion'].value_counts(normalize=True).plot(kind='bar', ax=axs[1], color='blue')\n",
    "axs[1].set_title('Emotion Distribution')\n",
    "axs[1].set_ylabel('Normalized Count')\n",
    "axs[1].set_xticklabels(split_labels(df_merged['emotion'].value_counts().index), rotation=45)\n",
    "\n",
    "# Histogram for topic\n",
    "df_merged['topic'].value_counts(normalize=True).plot(kind='bar', ax=axs[2], color='red')\n",
    "axs[2].set_title('Topic Distribution')\n",
    "axs[2].set_ylabel('Normalized Count')\n",
    "axs[2].set_xticklabels(split_labels(df_merged['topic'].value_counts().index), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('normalized_count_distributions.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group aggregated ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triple_stats = df_merged.groupby(['emotion', 'sentiment', 'topic']).agg(mean_ctr=('ctr', 'mean'), counts=('ctr', 'size')).reset_index()\n",
    "total_counts = triple_stats['counts'].sum()\n",
    "\n",
    "# Normalize the counts\n",
    "triple_stats['group_normalized_counts'] = triple_stats['counts'] / total_counts\n",
    "triple_stats = triple_stats.sort_values(by='mean_ctr',ascending=False)\n",
    "# Display the results\n",
    "triple_stats['rank_aggregated_triplet'] = triple_stats['mean_ctr'].rank(method='dense', ascending=False).astype(int)\n",
    "triple_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual articles (not aggregated), top performing, maybe one article popular but not the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the top 5% cutoff\n",
    "top_5_percent_cutoff = int(len(df_merged) * 0.05)\n",
    "\n",
    "# Select the top 5% articles by CTR\n",
    "top_crt_df = df_merged.sort_values(by='ctr', ascending=False).head(top_5_percent_cutoff)\n",
    "\n",
    "# Group by 'emotion', 'sentiment', and 'topic', and count the occurrences\n",
    "triplet_counts = top_crt_df.groupby(['emotion', 'sentiment', 'topic']).size().reset_index(name='counts')\n",
    "\n",
    "# Calculate the total number of entries in top_crt_df\n",
    "total_counts =  len(top_crt_df)\n",
    "\n",
    "# Normalize the counts\n",
    "triplet_counts['ind_normalized_counts'] = triplet_counts['counts'] / total_counts\n",
    "\n",
    "# Sort the data for better readability (optional)\n",
    "triplet_counts = triplet_counts.sort_values(by='ind_normalized_counts', ascending=False)\n",
    "\n",
    "# Display the normalized triplet counts\n",
    "triplet_counts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add a rank column based on 'normalized_counts'\n",
    "triplet_counts['rank_individual_occurence'] = triplet_counts['ind_normalized_counts'].rank(method='dense', ascending=False).astype(int)\n",
    "triplet_counts_individual = triplet_counts.copy()\n",
    "triplet_counts_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dailly aggregates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aggregations = {\n",
    "    'impressions': 'sum',\n",
    "    'clicks': 'sum',\n",
    "    'ctr':'mean'\n",
    "}\n",
    "non_numeric_columns = ['test_id', 'cleaned_text','emotion','sentiment','topic']\n",
    "aggregations.update({col: 'first' for col in non_numeric_columns})\n",
    "\n",
    "# Group by 'created_at' and apply the aggregation\n",
    "df_daily_agg = df_merged.groupby('created_at').agg(aggregations).reset_index()\n",
    "df_daily_agg = df_daily_agg.sort_values(by='created_at')\n",
    "\n",
    "# Ensure 'created_at' is a datetime\n",
    "df_daily_agg['created_at'] = pd.to_datetime(df_daily_agg['created_at'])\n",
    "\n",
    "# Create a complete datetime index\n",
    "complete_date_range = pd.date_range(start=df_daily_agg['created_at'].min(), \n",
    "                                    end=df_daily_agg['created_at'].max(), \n",
    "                                    freq='D')\n",
    "\n",
    "# Reindex the DataFrame - making sure that 'created_at' is the index\n",
    "df_daily_agg_reindexed = df_daily_agg.set_index('created_at').reindex(complete_date_range, method=None).rename_axis('created_at').reset_index()\n",
    "\n",
    "# Forward fill missing values\n",
    "df_daily_agg_filled = df_daily_agg_reindexed.ffill()\n",
    "\n",
    "# Calculate daily CTR\n",
    "df_daily_agg_filled['daily_ctr'] = df_daily_agg_filled['clicks'] / df_daily_agg_filled['impressions']\n",
    "\n",
    "# Check final DataFrame\n",
    "df_daily_agg = df_daily_agg_filled\n",
    "df_daily_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'created_at' and 'triplet' (emotion, sentiment, topic), then sum the clicks and impressions for each group\n",
    "grouped = df_merged.groupby(['created_at', 'emotion', 'sentiment', 'topic']).agg({'clicks': 'sum', 'impressions': 'sum'}).reset_index()\n",
    "\n",
    "# Compute the CTR for each group\n",
    "grouped['daily_group_ctr'] = grouped['clicks'] / grouped['impressions']\n",
    "\n",
    "# Sort the groups by date and CTR, then select the top triplet for each day\n",
    "top_triplet_each_day = grouped.sort_values(['created_at', 'daily_group_ctr'], ascending=[True, False]).groupby('created_at').head(1)\n",
    "\n",
    "# Now you can proceed with counting, normalizing, and ranking these triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count how many times each triplet appears\n",
    "triplet_counts = top_triplet_each_day.groupby(['emotion', 'sentiment', 'topic']).size().reset_index(name='counts')\n",
    "\n",
    "# Calculate normalized counts\n",
    "total_counts = triplet_counts['counts'].sum()\n",
    "triplet_counts['daily_normalized_counts'] = triplet_counts['counts'] / total_counts\n",
    "\n",
    "# Sort the triplets by normalized counts\n",
    "triplet_counts_sorted = triplet_counts.sort_values(by='daily_normalized_counts', ascending=False)\n",
    "\n",
    "# Rank the triplets\n",
    "triplet_counts_sorted['rank_daily_top_contribution'] = triplet_counts_sorted['daily_normalized_counts'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "# Display the sorted and ranked triplets\n",
    "triplet_counts_daily = triplet_counts_sorted\n",
    "triplet_counts_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# Merge the first two dataframes\n",
    "merged_df = pd.merge(triple_stats, triplet_counts_individual, on=['emotion', 'sentiment', 'topic'], how='inner')\n",
    "\n",
    "# Now merge the result with the third dataframe\n",
    "triple_ranks = pd.merge(merged_df, triplet_counts_daily, on=['emotion', 'sentiment', 'topic'], how='inner')\n",
    "\n",
    "# Selecting specific columns and sorting\n",
    "triple_ranks = triple_ranks[['emotion', 'sentiment', 'topic', 'rank_individual_occurence', 'rank_aggregated_triplet', 'rank_daily_top_contribution','daily_normalized_counts']].sort_values(by='rank_individual_occurence')\n",
    "\n",
    "# Display the DataFrame\n",
    "\n",
    "# Mean clickbait rank\n",
    "\n",
    "triple_ranks['mean_clickbait_rank'] = triple_ranks[['rank_individual_occurence', 'rank_aggregated_triplet', 'rank_daily_top_contribution']].mean(axis=1)\n",
    "\n",
    "# Sort the DataFrame by the new combined rank\n",
    "triple_ranks_sorted = triple_ranks.sort_values(by='mean_clickbait_rank')\n",
    "\n",
    "# Select specific columns and sort\n",
    "# Assuming you want to include new columns like 'rank_peaks', 'normalized_counts_peaks', etc.\n",
    "selected_columns = [\n",
    "    'emotion', 'sentiment', 'topic', 'mean_clickbait_rank',\n",
    "    'rank_individual_occurence',\n",
    "    'rank_aggregated_triplet',\n",
    "    'rank_daily_top_contribution', \n",
    "]\n",
    "\n",
    "triple_ranks = triple_ranks[selected_columns].sort_values(by='mean_clickbait_rank')\n",
    "\n",
    "# Display the DataFrame\n",
    "# Assuming your DataFrame is named triple_ranks\n",
    "# Add a new column for the combined rank\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "triple_ranks.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming triple_ranks is your DataFrame\n",
    "columns = ['rank_individual_occurence', 'rank_aggregated_triplet', 'rank_daily_top_contribution']\n",
    "correlation_matrix = pd.DataFrame(index=columns, columns=columns)\n",
    "\n",
    "# Calculate Spearman rank correlation and check for significance\n",
    "for col1 in columns:\n",
    "    for col2 in columns:\n",
    "        corr, p_value = spearmanr(triple_ranks[col1], triple_ranks[col2])\n",
    "        # Mark with two asterisks if p-value is less than 0.01, one asterisk if less than 0.05\n",
    "        if p_value < 0.01:\n",
    "            correlation_matrix.at[col1, col2] = f\"{corr:.2f}**\"\n",
    "        elif p_value < 0.05:\n",
    "            correlation_matrix.at[col1, col2] = f\"{corr:.2f}*\"\n",
    "        else:\n",
    "            correlation_matrix.at[col1, col2] = f\"{corr:.2f}\"\n",
    "\n",
    "# Display the correlation matrix\n",
    "correlation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ones are hit or miss that are daily performers typically\n",
    "# But the groups show that on average, they drive consistent traffic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming triple_ranks is your DataFrame with 'emotion', 'sentiment', 'topic', and 'mean_clickbait_rank'\n",
    "\n",
    "# Grouping by emotion\n",
    "grouped_emotion = triple_ranks.groupby('emotion')['mean_clickbait_rank'].mean().reset_index()\n",
    "std_emotion = triple_ranks.groupby('emotion')['mean_clickbait_rank'].std().reset_index()\n",
    "\n",
    "# Grouping by sentiment\n",
    "grouped_sentiment = triple_ranks.groupby('sentiment')['mean_clickbait_rank'].mean().reset_index()\n",
    "std_sentiment = triple_ranks.groupby('sentiment')['mean_clickbait_rank'].std().reset_index()\n",
    "\n",
    "# Grouping by topic\n",
    "grouped_topic = triple_ranks.groupby('topic')['mean_clickbait_rank'].mean().reset_index()\n",
    "std_topic = triple_ranks.groupby('topic')['mean_clickbait_rank'].std().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Assuming triple_ranks is your DataFrame\n",
    "categories = ['emotion', 'sentiment', 'topic']\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    \n",
    "    # Get unique group names for the current category\n",
    "    group_names = triple_ranks[category].unique()\n",
    "\n",
    "    # Normality test for each group\n",
    "    print(\"Shapiro-Wilk Test for Normality:\")\n",
    "    for group in group_names:\n",
    "        group_data = triple_ranks[triple_ranks[category] == group]['mean_clickbait_rank']\n",
    "        shapiro_test = stats.shapiro(group_data)\n",
    "        print(f\"{group}: Statistics={shapiro_test.statistic}, p-value={shapiro_test.pvalue}\")\n",
    "        if shapiro_test.pvalue < 0.05:\n",
    "            print(f\"  - The data for {group} in {category} is not normally distributed.\")\n",
    "        else:\n",
    "            print(f\"  - The data for {group} in {category} appears to be normally distributed.\")\n",
    "\n",
    "    # Homogeneity of variances test across groups\n",
    "    print(\"\\nLevene's Test for Homogeneity of Variances:\")\n",
    "    levene_test = stats.levene(*[triple_ranks[triple_ranks[category] == group]['mean_clickbait_rank'] for group in group_names])\n",
    "    print(f\"Statistics={levene_test.statistic}, p-value={levene_test.pvalue}\")\n",
    "    if levene_test.pvalue < 0.05:\n",
    "        print(f\"  - There is evidence to suggest that variances are not equal across the {category} groups.\")\n",
    "    else:\n",
    "        print(f\"  - Variances appear to be equal across the {category} groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Assuming triple_ranks is your DataFrame\n",
    "categories = ['emotion', 'sentiment', 'topic']\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "\n",
    "    # Extracting the groups for the current category\n",
    "    groups = [group['mean_clickbait_rank'].values for name, group in triple_ranks.groupby(category)]\n",
    "\n",
    "    # Kruskal-Wallis H Test\n",
    "    kw_result = stats.kruskal(*groups)\n",
    "    print(f\"Kruskal-Wallis H Test for {category}: Statistics={kw_result.statistic}, p-value={kw_result.pvalue}\")\n",
    "\n",
    "    # Interpret the results\n",
    "    if kw_result.pvalue < 0.05:\n",
    "        print(\"  - There is a statistically significant difference between groups in this category.\")\n",
    "    else:\n",
    "        print(\"  - No statistically significant difference was found between groups in this category.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define color mappings\n",
    "emotion_colors = {'anger': 'red', 'joy': 'yellow', 'sadness': 'blue', 'fear': 'black', 'surprise': 'orange','love':'pink', 'unclassified_emotion':'gray'}\n",
    "sentiment_colors = {'positive': 'green', 'negative': 'red', 'unclassified sentiment': 'gray'}\n",
    "topic_colors = {'Authenticity, Lifestyle and Health': 'green', \n",
    "                'Society and Social Issues': 'cyan', \n",
    "                'Pop Culture, Media and Entertainment': 'magenta', \n",
    "                'Relationships, Gender and Family':'red',\n",
    "                'unclassified topic': 'gray'}\n",
    "\n",
    "# Function to get colors for each bar\n",
    "def get_colors(grouped_data, color_map, column_name):\n",
    "    colors = [color_map.get(x, 'gray') for x in grouped_data[column_name]]\n",
    "    return colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming triple_ranks is your DataFrame and previous tests have been run\n",
    "categories = ['emotion', 'sentiment', 'topic']\n",
    "kw_results = {\n",
    "    'emotion': {\"stat\": 36.8467, \"p_value\": 1.8863e-06},\n",
    "    'sentiment': {\"stat\": 7.8058, \"p_value\": 0.0202},\n",
    "    'topic': {\"stat\": 7.1258, \"p_value\": 0.1294}\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 18), gridspec_kw={'height_ratios': [1, 1, 1]})\n",
    "\n",
    "# Plot for each category\n",
    "for i, category in enumerate(categories):\n",
    "    grouped_data = triple_ranks.groupby(category)['mean_clickbait_rank'].mean().reset_index()\n",
    "    std_data = triple_ranks.groupby(category)['mean_clickbait_rank'].std().reset_index()\n",
    "\n",
    "    # Define color mappings (update these as per your categories)\n",
    "    color_mappings = {\n",
    "        'emotion': emotion_colors,\n",
    "        'sentiment': sentiment_colors,\n",
    "        'topic': topic_colors\n",
    "    }\n",
    "\n",
    "    bars = axs[i].bar(grouped_data[category], grouped_data['mean_clickbait_rank'], yerr=std_data['mean_clickbait_rank'], color=get_colors(grouped_data, color_mappings[category], category), alpha=0.8)\n",
    "    axs[i].set_title(f'Mean Clickbait Rank by {category.capitalize()}')\n",
    "    axs[i].set_ylabel('Mean Clickbait Rank')\n",
    "    axs[i].set_xticks(range(len(grouped_data[category])))\n",
    "    axs[i].set_xticklabels(grouped_data[category], rotation=30)\n",
    "\n",
    "    # Preparing Kruskal-Wallis H Test results for the legend\n",
    "    kw_result = kw_results[category]\n",
    "    significant = kw_result[\"p_value\"] < 0.05\n",
    "    symbol = \"‚úì\" if significant else \"X\"\n",
    "    stat_text = f'Kruskal-Wallis H: Stat={kw_result[\"stat\"]:.2f}, p={kw_result[\"p_value\"]:.2e}\\nSignificance: {symbol}'\n",
    "    \n",
    "    # Create a patch for the legend\n",
    "    patch = mpatches.Patch(color='white', label=stat_text)\n",
    "    \n",
    "    # Adding the test result to the legend\n",
    "    axs[i].legend(handles=[patch], loc='upper right', fontsize=16, frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('triplets_grouped.pdf', format='pdf', bbox_inches='tight', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null model for ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_triple_stats(df):\n",
    "    triple_stats = df.groupby(['emotion', 'sentiment', 'topic']).agg(mean_ctr=('ctr', 'mean'), counts=('ctr', 'size')).reset_index()\n",
    "    total_counts = triple_stats['counts'].sum()\n",
    "\n",
    "    # Normalize the counts\n",
    "    triple_stats['group_normalized_counts'] = triple_stats['counts'] / total_counts\n",
    "    triple_stats = triple_stats.sort_values(by='mean_ctr', ascending=False)\n",
    "\n",
    "    # Rank the results\n",
    "    triple_stats['rank_aggregated_triplet'] = triple_stats['mean_ctr'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "    return triple_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_triplet_counts_individual(df):\n",
    "    # Calculate the top 5% cutoff\n",
    "    top_5_percent_cutoff = int(len(df) * 0.05)\n",
    "\n",
    "    # Select the top 5% articles by CTR\n",
    "    top_crt_df = df.sort_values(by='ctr', ascending=False).head(top_5_percent_cutoff)\n",
    "\n",
    "    # Group by 'emotion', 'sentiment', and 'topic', and count the occurrences\n",
    "    triplet_counts = top_crt_df.groupby(['emotion', 'sentiment', 'topic']).size().reset_index(name='counts')\n",
    "\n",
    "    # Calculate the total number of entries in top_crt_df\n",
    "    total_counts = len(top_crt_df)\n",
    "\n",
    "    # Normalize the counts\n",
    "    triplet_counts['ind_normalized_counts'] = triplet_counts['counts'] / total_counts\n",
    "\n",
    "    # Sort the data for better readability (optional)\n",
    "    triplet_counts = triplet_counts.sort_values(by='ind_normalized_counts', ascending=False)\n",
    "\n",
    "    # Add a rank column based on 'normalized_counts'\n",
    "    triplet_counts['rank_individual_occurence'] = triplet_counts['ind_normalized_counts'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "    return triplet_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_triplet_counts_daily(df):\n",
    "    aggregations = {\n",
    "        'impressions': 'sum',\n",
    "        'clicks': 'sum',\n",
    "        'ctr': 'mean'\n",
    "    }\n",
    "    non_numeric_columns = ['test_id', 'cleaned_text', 'emotion', 'sentiment', 'topic']\n",
    "    aggregations.update({col: 'first' for col in non_numeric_columns})\n",
    "\n",
    "    # Group by 'created_at' and apply the aggregation\n",
    "    df_daily_agg = df.groupby('created_at').agg(aggregations).reset_index()\n",
    "    df_daily_agg = df_daily_agg.sort_values(by='created_at')\n",
    "\n",
    "    # Ensure 'created_at' is a datetime\n",
    "    df_daily_agg['created_at'] = pd.to_datetime(df_daily_agg['created_at'])\n",
    "\n",
    "    # Create a complete datetime index\n",
    "    complete_date_range = pd.date_range(start=df_daily_agg['created_at'].min(), \n",
    "                                        end=df_daily_agg['created_at'].max(), \n",
    "                                        freq='D')\n",
    "\n",
    "    # Reindex the DataFrame - making sure that 'created_at' is the index\n",
    "    df_daily_agg_reindexed = df_daily_agg.set_index('created_at').reindex(complete_date_range, method=None).rename_axis('created_at').reset_index()\n",
    "\n",
    "    # Forward fill missing values\n",
    "    df_daily_agg_filled = df_daily_agg_reindexed.ffill()\n",
    "\n",
    "    # Calculate daily CTR\n",
    "    df_daily_agg_filled['daily_ctr'] = df_daily_agg_filled['clicks'] / df_daily_agg_filled['impressions']\n",
    "\n",
    "    # Group by 'created_at' and 'triplet' (emotion, sentiment, topic), then sum the clicks and impressions for each group\n",
    "    grouped = df.groupby(['created_at', 'emotion', 'sentiment', 'topic']).agg({'clicks': 'sum', 'impressions': 'sum'}).reset_index()\n",
    "\n",
    "    # Compute the CTR for each group\n",
    "    grouped['daily_group_ctr'] = grouped['clicks'] / grouped['impressions']\n",
    "\n",
    "    # Sort the groups by date and CTR, then select the top triplet for each day\n",
    "    top_triplet_each_day = grouped.sort_values(['created_at', 'daily_group_ctr'], ascending=[True, False]).groupby('created_at').head(1)\n",
    "\n",
    "    # Count how many times each triplet appears\n",
    "    triplet_counts = top_triplet_each_day.groupby(['emotion', 'sentiment', 'topic']).size().reset_index(name='counts')\n",
    "\n",
    "    # Calculate normalized counts\n",
    "    total_counts = triplet_counts['counts'].sum()\n",
    "    triplet_counts['daily_normalized_counts'] = triplet_counts['counts'] / total_counts\n",
    "\n",
    "    # Sort the triplets by normalized counts\n",
    "    triplet_counts_sorted = triplet_counts.sort_values(by='daily_normalized_counts', ascending=False)\n",
    "\n",
    "    # Rank the triplets\n",
    "    triplet_counts_sorted['rank_daily_top_contribution'] = triplet_counts_sorted['daily_normalized_counts'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "    return triplet_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_final_triple_ranks(df):\n",
    "    # Call the previously defined functions to get the required dataframes\n",
    "    triple_stats = calculate_triple_stats(df)\n",
    "    triplet_counts_individual = calculate_triplet_counts_individual(df)\n",
    "    triplet_counts_daily = calculate_triplet_counts_daily(df)\n",
    "\n",
    "    # Determine the set of all unique triplets across all dataframes\n",
    "    all_triplets = set(triple_stats[['emotion', 'sentiment', 'topic']].itertuples(index=False)) | \\\n",
    "                   set(triplet_counts_individual[['emotion', 'sentiment', 'topic']].itertuples(index=False)) | \\\n",
    "                   set(triplet_counts_daily[['emotion', 'sentiment', 'topic']].itertuples(index=False))\n",
    "\n",
    "    # Create a DataFrame of all unique triplets\n",
    "    all_triplets_df = pd.DataFrame(all_triplets, columns=['emotion', 'sentiment', 'topic'])\n",
    "\n",
    "    # Merge the dataframes with all_triplets_df to ensure all triplets are represented\n",
    "    merged_df = pd.merge(all_triplets_df, triple_stats, on=['emotion', 'sentiment', 'topic'], how='left')\n",
    "    merged_df = pd.merge(merged_df, triplet_counts_individual, on=['emotion', 'sentiment', 'topic'], how='left')\n",
    "    merged_df = pd.merge(merged_df, triplet_counts_daily, on=['emotion', 'sentiment', 'topic'], how='left')\n",
    "\n",
    "    # Assign last ranks for missing values in each ranking column\n",
    "    max_rank_individual = merged_df['rank_individual_occurence'].max() + 1\n",
    "    max_rank_aggregated = merged_df['rank_aggregated_triplet'].max() + 1\n",
    "    max_rank_daily = merged_df['rank_daily_top_contribution'].max() + 1\n",
    "\n",
    "    merged_df['rank_individual_occurence'].fillna(max_rank_individual, inplace=True)\n",
    "    merged_df['rank_aggregated_triplet'].fillna(max_rank_aggregated, inplace=True)\n",
    "    merged_df['rank_daily_top_contribution'].fillna(max_rank_daily, inplace=True)\n",
    "\n",
    "    # Calculate the mean clickbait rank\n",
    "    merged_df['mean_clickbait_rank'] = merged_df[['rank_individual_occurence', 'rank_aggregated_triplet', 'rank_daily_top_contribution']].mean(axis=1)\n",
    "\n",
    "    # Sort the DataFrame by the new combined rank\n",
    "    triple_ranks_sorted = merged_df.sort_values(by='mean_clickbait_rank')\n",
    "\n",
    "    return triple_ranks_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def calculate_z_test(original_mean, shuffled_means, std_dev, n):\n",
    "    z_score = (original_mean - np.mean(shuffled_means)) / (std_dev / np.sqrt(n))\n",
    "    p_value = stats.norm.sf(abs(z_score)) * 2\n",
    "    return p_value\n",
    "\n",
    "def calculate_mannwhitney_u_test(original_data, shuffled_data):\n",
    "    u_statistic, p_value = stats.mannwhitneyu(original_data, shuffled_data, alternative='two-sided')\n",
    "    return p_value\n",
    "\n",
    "# Assuming df_merged is your original dataframe and calculate_final_triple_ranks is defined\n",
    "original_ranks = calculate_final_triple_ranks(df_merged)\n",
    "\n",
    "# Prepare to store results\n",
    "results = []\n",
    "\n",
    "# Bonferroni correction\n",
    "alpha = 0.05 / 2 #Two tests per triplet\n",
    "trials = 1000\n",
    "\n",
    "# Perform shuffling for each trial\n",
    "triplet_shuffled_means, ctr_shuffled_means = {}, {}\n",
    "\n",
    "# Perform shuffling for each trial\n",
    "for i in range(trials):\n",
    "    print(i, trials)\n",
    "\n",
    "    # Shuffle for triplet and ctr\n",
    "    df_shuffle_triplet = df_merged.copy()\n",
    "    df_shuffle_triplet['sentiment'] = df_shuffle_triplet['sentiment'].sample(frac=1).reset_index(drop=True)\n",
    "    df_shuffle_triplet['emotion'] = df_shuffle_triplet['emotion'].sample(frac=1).reset_index(drop=True)\n",
    "    df_shuffle_triplet['topic'] = df_shuffle_triplet['topic'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    df_shuffle_ctr = df_merged.copy()\n",
    "    df_shuffle_ctr['ctr'] = df_shuffle_ctr['ctr'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    shuffled_rank_triplet = calculate_final_triple_ranks(df_shuffle_triplet)\n",
    "    shuffled_rank_ctr = calculate_final_triple_ranks(df_shuffle_ctr)\n",
    "\n",
    "    # Iterate over each triplet in the original ranks\n",
    "    for index, row in original_ranks.iterrows():\n",
    "        emotion, sentiment, topic = row['emotion'], row['sentiment'], row['topic']\n",
    "        triplet_key = (emotion, sentiment, topic)\n",
    "\n",
    "        shuffled_mean_triplet = shuffled_rank_triplet.loc[(shuffled_rank_triplet['emotion'] == emotion) & \n",
    "                                                          (shuffled_rank_triplet['sentiment'] == sentiment) & \n",
    "                                                          (shuffled_rank_triplet['topic'] == topic), 'mean_clickbait_rank'].mean()\n",
    "        shuffled_mean_ctr = shuffled_rank_ctr.loc[(shuffled_rank_ctr['emotion'] == emotion) & \n",
    "                                                  (shuffled_rank_ctr['sentiment'] == sentiment) & \n",
    "                                                  (shuffled_rank_ctr['topic'] == topic), 'mean_clickbait_rank'].mean()\n",
    "\n",
    "        triplet_shuffled_means.setdefault(triplet_key, []).append(shuffled_mean_triplet)\n",
    "        ctr_shuffled_means.setdefault(triplet_key, []).append(shuffled_mean_ctr)\n",
    "\n",
    "# Perform Shapiro-Wilk test and choose appropriate test (Z-Test or Mann-Whitney)\n",
    "for triplet_key in original_ranks[['emotion','sentiment','topic']].values:\n",
    "    triplet_key = tuple(triplet_key)  # Convert numpy.ndarray to tuple\n",
    "    emotion, sentiment, topic = triplet_key\n",
    "    original_mean = original_ranks.loc[(original_ranks['emotion'] == emotion) & \n",
    "                                       (original_ranks['sentiment'] == sentiment) & \n",
    "                                       (original_ranks['topic'] == topic), 'mean_clickbait_rank'].mean()\n",
    "\n",
    "    for shuffled_means, shuffle_type in [(triplet_shuffled_means[triplet_key], 'Triplet Shuffle'), (ctr_shuffled_means[triplet_key], 'CTR Shuffle')]:\n",
    "        if len(shuffled_means) >= 3:\n",
    "            shapiro_test = stats.shapiro(shuffled_means)\n",
    "            p_value_shapiro = shapiro_test.pvalue\n",
    "\n",
    "            if p_value_shapiro >= alpha:\n",
    "                p_value_test = calculate_z_test(original_mean, shuffled_means, np.std(shuffled_means), len(shuffled_means))\n",
    "                test_type = 'Z-Test'\n",
    "            else:\n",
    "                p_value_test = calculate_mannwhitney_u_test([original_mean], shuffled_means)\n",
    "                test_type = 'Mann-Whitney U'\n",
    "\n",
    "            results.append({\n",
    "                'emotion': emotion,\n",
    "                'sentiment': sentiment,\n",
    "                'topic': topic,\n",
    "                'shuffle_type': shuffle_type,\n",
    "                'shapiro_p_value': p_value_shapiro,\n",
    "                'test_type': test_type,\n",
    "                'p_value_test': p_value_test\n",
    "            })\n",
    "        else:\n",
    "            pass\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['stat_significant'] = results_df.apply(lambda x: 'Statistically Significant' if x['p_value_test']<alpha else 'Not Statistically Significant',axis=1)\n",
    "# Print and save the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['stat_significant'] = results_df.apply(lambda x: 'Statistically Significant' if x['p_value_test']<alpha else 'Not Statistically Significant',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recreating the plots with logarithmic scale on the x-axis\n",
    "\n",
    "# Filter out rows with p-values of zero or NaN\n",
    "filtered_results_df = results_df[(results_df['p_value_test'] > 0) & (results_df['p_value_test'].notna())]\n",
    "\n",
    "# Recalculate the filtered DataFrames and counts\n",
    "triplet_shuffle = filtered_results_df[filtered_results_df['shuffle_type'] == 'Triplet Shuffle']\n",
    "ctr_shuffle = filtered_results_df[filtered_results_df['shuffle_type'] == 'CTR Shuffle']\n",
    "triplet_counts = triplet_shuffle['stat_significant'].value_counts()\n",
    "ctr_counts = ctr_shuffle['stat_significant'].value_counts()\n",
    "\n",
    "# Define alpha threshold for statistical significance\n",
    "alpha_threshold = alpha\n",
    "\n",
    "min_p_value = max(1e-10, min(filtered_results_df['p_value_test']))\n",
    "max_p_value = max(filtered_results_df['p_value_test'])\n",
    "bin_edges = np.logspace(np.log10(min_p_value), np.log10(max_p_value), 20)\n",
    "\n",
    "# Create the plots with adjusted bin ranges and log scale for x-axis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "\n",
    "# Plot histograms with color based on statistical significance\n",
    "sns.histplot(triplet_shuffle, x='p_value_test', hue='stat_significant', \n",
    "             palette={'Statistically Significant': 'green', 'Not Statistically Significant': 'red'}, \n",
    "             ax=axes[0], kde=False, bins=20, log_scale=True)\n",
    "sns.histplot(ctr_shuffle, x='p_value_test', hue='stat_significant', \n",
    "             palette={'Statistically Significant': 'green', 'Not Statistically Significant': 'red'}, \n",
    "             ax=axes[1], kde=False, bins=20, log_scale=True)\n",
    "\n",
    "# Add alpha threshold lines\n",
    "axes[0].axvline(alpha_threshold, color='blue', linestyle='--')\n",
    "axes[1].axvline(alpha_threshold, color='blue', linestyle='--')\n",
    "\n",
    "# Set titles and labels\n",
    "axes[0].set_title('Triplet Shuffle P-Values')\n",
    "axes[0].set_xlabel('P-Value')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].set_title('CTR Shuffle P-Values')\n",
    "axes[1].set_xlabel('P-Value')\n",
    "\n",
    "# Create custom legend entries\n",
    "sig_patch = mpatches.Patch(color='none', label=f'Significant: {triplet_counts.get(\"Statistically Significant\", 0)}')\n",
    "not_sig_patch = mpatches.Patch(color='none', label=f'Not Significant: {triplet_counts.get(\"Not Statistically Significant\", 0)}')\n",
    "alpha_patch = mpatches.Patch(color='blue', label='Alpha Threshold')\n",
    "\n",
    "# Add legends with custom entries\n",
    "axes[0].legend(handles=[alpha_patch, sig_patch, not_sig_patch], title='Triplet Shuffle')\n",
    "axes[1].legend(handles=[alpha_patch, sig_patch, not_sig_patch], title='CTR Shuffle')\n",
    "\n",
    "# Remove grid\n",
    "axes[0].grid(False)\n",
    "axes[1].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('null_model_results.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
